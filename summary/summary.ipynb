{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a68463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RocketLinksDE/opt/anaconda3/envs/final_project/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4bd49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████| 780k/780k [00:00<00:00, 1.15MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████| 446k/446k [00:00<00:00, 752kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████| 239/239 [00:00<00:00, 82.6kB/s]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7310a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ''' Project lead: What do you think about our plans for this product launch?\n",
    "\n",
    "Colleague: It looks to me like you have a lot planned before your deadline. I would suggest you push your deadline back so you have time to run a successful advertising campaign.\n",
    "\n",
    "Project lead: I respectfully disagree with you there. The priority is to launch before the holidays, so we do not want to move this deadline.\n",
    "\n",
    "Colleague: I would suggest you discuss this further with the advertising team.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f357073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but you input_length is only 105. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The project lead wants to push back the deadline for the product launch. The priority is to launch before the holidays, so they do not want to move the deadline. The project lead will discuss this further with the advertising team.   . \\xa0 '}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2a6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36e700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36ec045",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = ''' \"The evaluation of a summary quality is a very ambitious task. Serious questionsremain concerning the appropriate methods and types of evaluation. There are a va-riety of possible bases for the comparison of summarization systems performance.We can compare a system summary to the source text, to a human-generated sum-mary or to another system summary. Summarization evaluation methods can bebroadly classiﬁed into two categories [37]. In extrinsic evaluation, the summaryquality is judged on the basis of how helpful summaries are for a given task, and inintrinsic evaluation, it is directly based on analysis of the summary. The latter caninvolve a comparison with the source document, measuring how many main ideasof the source document are covered by the summary or a content comparison withan abstract written by a human. The problem of matching the system summaryagainst an “ideal summary” is that the ideal summary is hard to establish. Thehuman summary may be supplied by the author of the article, by a judge askedto construct an abstract, or by a judge asked to extract sentences. There can bea large number of abstracts that can summarize a given document. The intrinsicevaluations can then be broadly divided into content evaluation and text quality eva-luation. Whereas content evaluations measure the ability to identify the key topics,text quality evaluations judge the readability, grammar and coherence of automaticsummaries\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333606de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"The evaluation of a summary quality is a very ambitious task. Serious questionsremain concerning the appropriate methods and types of evaluation. There are a va-riety of possible bases for the comparison of summarization systems performance.We can compare a system summary to the source text, to a human-generated sum-mary or to another system summary. Summarization evaluation methods can bebroadly classiﬁed into two categories [37]. In extrinsic evaluation, the summaryquality is judged on the basis of how helpful summaries are for a given task, and inintrinsic evaluation, it is directly based on analysis of the summary. The latter caninvolve a comparison with the source document, measuring how many main ideasof the source document are covered by the summary or a content comparison withan abstract written by a human. The problem of matching the system summaryagainst an “ideal summary” is that the ideal summary is hard to establish. Thehuman summary may be supplied by the author of the article, by a judge askedto construct an abstract, or by a judge asked to extract sentences. There can bea large number of abstracts that can summarize a given document. The intrinsicevaluations can then be broadly divided into content evaluation and text quality eva-luation. Whereas content evaluations measure the ability to identify the key topics,text quality evaluations judge the readability, grammar and coherence of automaticsummaries\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e974d12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'The evaluation of a summary quality is a very ambitious task. There are a number of possible bases for the comparison of summarization systems performance. In extrinsic evaluation, the summaryquality is judged on the basis of how helpful summaries are for a given task. In Intrinsic evaluations, it is directly based on analysis of the summary. The latter can involve a comparison with the source document.'}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36dae650",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./summarizer_bart_large_cnn_samsum.pkl', 'wb') as file:\n",
    "    pickle.dump(summarizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5cb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e7b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./summarizer_bart_large_cnn_samsum.pkl', 'rb') as file:\n",
    "    summarizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dcbc9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The evaluation of a summary quality is a very ambitious task. There are a number of possible bases for the comparison of summarization systems performance. In extrinsic evaluation, the summaryquality is judged on the basis of how helpful summaries are for a given task. In Intrinsic evaluations, it is directly based on analysis of the summary. The latter can involve a comparison with the source document.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0010b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_2 = '''\"Ravi: Hey Garima! How are you doing today?\n",
    "\n",
    "Garima: I am good. Just working on my final project. How are you doing?\n",
    "\n",
    "\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b90eeb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummarizer\u001b[49m(conversation_2, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summarizer' is not defined"
     ]
    }
   ],
   "source": [
    "summarizer(conversation_2, max_length=80, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f71a841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea1111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "final_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
